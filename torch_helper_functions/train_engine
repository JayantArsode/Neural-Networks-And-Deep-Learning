"""
Helper class for training and validation steps in a convolutional neural network
Also it will save all model progress in weight and bias platform.

## Attributes:
  None
## Methods:
  `train_step`: Performs a single training step.
  `validation_step`: Performs a single validation step.
  `train`: Trains the model for a specified number of epochs.
"""
from torch.utils.data import DataLoader
from torch import nn,optim,cuda,softmax,onnx
from torchmetrics import metrics
from tqdm.auto import tqdm
from torchmetrics import Accuracy,F1Score
from typing import Tuple
import wandb

def train_step(model: nn.Module,
               dataloader: DataLoader,
               loss_fn: nn.Module,
               accuracy_fn:Accuracy,
               f1_score_fn:F1Score,
               optimizer: optim.Optimizer)->Tuple[int,int,int]:
    """
    #### Perform a single training step.

    ## Parameters:
        `model (nn.Module)`: The neural network model.
        `dataloader (torch.utils.data.DataLoader)`: DataLoader for the training dataset.
        `loss_fn (nn.Module)`: Loss function.
        `accuracy_fn (torchmetrics.Accuracy)`: Function to calculate accuracy.
        `f1_score_fn (torchmetrics.F1Score)`: Function to calculate F1 score.
        `optimizer (torch.optim.Optimizer)`: Optimizer for gradient descent.

    ## Returns:
        tuple: Tuple containing training loss, accuracy, F1 score.

    ## Example:
        train_loss, train_acc, train_f1 = train_step(model, train_dataloader, loss_fn, accuracy_fn, f1_score_fn, optimizer)
        print(f"Training Loss: {train_loss}, Accuracy: {train_acc}, F1 Score: {train_f1}")
    """
    # Create train metrics variables
    train_loss, train_acc, train_f1 = 0, 0, 0
    device = "cuda" if cuda.is_available() else "cpu"
    # Creating training loop
    model.train()
    for batch, (X, y) in tqdm(enumerate(dataloader), desc="Training", leave=False, total=len(dataloader)):
        X, y = X.to(device), y.to(device)
        # 1. Forward pass
        y_logits = model(X)
        y_probs = softmax(y_logits, dim=1)
        y_preds = y_probs.argmax(dim=1)
        # 2. Calculate loss/acc
        loss = loss_fn(y_logits, y)
        train_loss += loss.item()
        train_acc += accuracy_fn(y_preds, y).item()
        train_f1 += f1_score_fn(y_preds, y).item()
        # 3. Zero grad
        optimizer.zero_grad()
        # 4. Backward pass
        loss.backward()
        # 5. Optimizer step
        optimizer.step()
    train_loss /= len(dataloader)
    train_acc /= len(dataloader)
    train_f1 /= len(dataloader)
    return train_loss, train_acc, train_f1


    def validation_step(model: nn.Module,
                    dataloader: DataLoader,
                    loss_fn: nn.Module,
                    accuracy_fn: Accuracy,
                    f1_score_fn: F1Score) -> Tuple[int,int,int]:
    """
    #### Perform a single validation step.

    ## Parameters:
        `model (nn.Module)`: The neural network model.
        `dataloader (torch.utils.data.DataLoader)`: DataLoader for the validation dataset.
        `accuracy_fn (torchmetrics.Accuracy)`: Function to calculate accuracy.
        `f1_score_fn (torchmetrics.F1Score)`: Function to calculate F1 score.
        `f1_score_fn`: Function to calculate F1 score.

    ## Returns:
        tuple: Tuple containing validation loss, accuracy, F1 score.

    ## Example:
        validation_step(model, validation_dataloader, loss_function, accuracy_function, f1_score_function, auc_roc_function)
    """
    # Create validation metrics variables
    val_loss, val_acc, val_f1 = 0, 0, 0
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # Creating validation loop
    model.eval()
    with torch.inference_mode():
        for batch, (X, y) in tqdm(enumerate(dataloader), desc="Validation", leave=False, total=len(dataloader)):
            X, y = X.to(device), y.to(device)
            # 1. Forward pass
            val_logits = model(X)
            val_probs = torch.softmax(val_logits, dim=1)
            val_preds = val_probs.argmax(dim=1)
            # 2. Calculate loss/acc
            loss = loss_fn(val_logits, y)
            val_loss += loss.item()
            val_acc += accuracy_fn(val_preds, y).item()
            val_f1 += f1_score_fn(val_preds, y).item()
            val_auc += auc_roc_fn(val_probs, y).item()
        val_loss /= len(dataloader)
        val_acc /= len(dataloader)
        val_f1 /= len(dataloader)
    return val_loss,val_acc,val_f1

def train(model: nn.Module,
          train_dataloader: DataLoader,
          val_dataloader: DataLoader,
          optimizer: optim.Optimizer,
          accuracy_fn:Accuracy,
          f1_score_fn:F1Score,
          lr_scheduler: optim.lr_scheduler,
          wandb_init_params:Dict={"project":"Demo_Project",
                                  "experiment":"Demo_Experiment",
                                  "hyperparameters":{
                                      "learning_rate":0.001,
                                      "epochs":5,
                                      "batch_size":32
                                  }},
          loss_fn: torch.nn.Module = nn.BCEWithLogitsLoss(),
          early_stopping: Dict = {"patience": 5},
          epochs: int = 5)->Dict:
    """
    #### Train the model for a specified number of epochs contains train_step and val_step.

    ## Parameters:
        `model (torch.nn.Module)`: The neural network model.
        `train_dataloader (torch.utils.data.DataLoader)`: DataLoader for the training dataset.
        `val_dataloader (torch.utils.data.DataLoader)`: DataLoader for the validation dataset.
        `optimizer (torch.optim.Optimizer)`: Optimizer for gradient descent.
        `accuracy_fn (torchmetrics.Accuracy)`: Function to calculate accuracy.
        `f1_score_fn (torchmetrics.F1Score)`: Function to calculate F1 score.
        `lr_scheduler (torch.optim.lr_scheduler)`: Learning rate scheduler. Default is None.
        `wandb_init_params (Dict)`: This contains the init function to use with wandb 
        `loss_fn (torch.nn.Module): Loss function. Default is BCEWithLogitsLoss.
        `early_stopping (Dict)`: Dictionary containing early stopping parameters. Default is {"patience": 5}.
        `epochs (int)`: Number of training epochs. Default is 5.

    ## Returns:
        dict: It return result dictionary containing info about like model_name:
              Training,Validation list of: loss,Accuracy,F1Score.
    ## Example:
        model_history=train(model,
                            train_dataloader,
                            val_dataloader,
                            optimizer,
                            accuracy_fn, f1_score_fn,
                            lr_scheduler,
                            wandb_init_params,
                            loss_fn,
                            early_stopping,
                            epochs)
    """
    results = {"model_name": wandb_init_params["experiment"],
                "train_loss": [],
                "train_acc": [],
                "train_f1_score": [],
                "val_loss": [],
                "val_acc": [],
                "val_f1_score": []
                }
    wandb.init(project=wandb_init_params["project"],
               exeperiment=wandb_init_params["experiment"],
               config=wandb_init_params["hyperparameters"])
    config = wandb.config
    # Log model architecture
    wandb.watch(model,crterion=loss_fn, log="all", log_freq=10)

    best_val_loss = float('inf')
    best_epoch = 0
    no_improvement = 0

    # Loop through training and validation steps for a number of epochs
    for epoch in tqdm(range(config.epochs), desc="Epochs"):
        # Training Step
        train_loss, train_acc, train_f1 = train_step(model, train_dataloader, loss_fn, accuracy_fn, f1_score_fn, optimizer)
        # Validation Step
        val_loss, val_acc, val_f1 = validation_step(model, val_dataloader, loss_fn, accuracy_fn, f1_score_fn, auc_roc_fn)
        # Step with validation loss for LR scheduler
        if lr_scheduler is not None:
            lr_scheduler.step(val_loss)

        print(f"Epoch: {epoch + 1} | "
              f"train_loss: {train_loss:.4f} | "
              f"train_acc: {train_acc:.4f} | "
              f"train_f1_score: {train_f1:.4f} | "
              f"val_loss: {val_loss:.4f} | "
              f"val_acc: {val_acc:.4f} | "
              f"val_f1_score: {val_f1:.4f}")

        # Update results dictionary
        wandb.log({
            "epoch": epoch + 1,
            "train_loss": train_loss,
            "train_acc": train_acc,
            "train_f1_score": train_f1,
            "val_loss": val_loss,
            "val_acc": val_acc,
            "val_f1_score": val_f1,
        })

        # Update results dictionary
        results["train_loss"].append(train_loss)
        results["train_acc"].append(train_acc)
        results["train_f1_score"].append(train_f1)
        results["val_loss"].append(val_loss)
        results["val_acc"].append(val_acc)
        results["val_f1_score"].append(val_f1)

        # Check for early stopping
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
            no_improvement = 0
        else:
            no_improvement += 1

        if no_improvement >= early_stopping["patience"]:
            wandb.log({"early_stopping_epoch": epoch + 1})
            wandb.log({"early_stopping_reason": "No improvement in validation loss"})
            print(f"Early stopping at epoch {epoch + 1}")
            break

    print(f"Best validation loss: {best_val_loss} at epoch {best_epoch + 1}")
    # Save the model in the exchangeable ONNX format
    onnx.export(model, images, f"{wandb_init_params['experiment']}.onnx")
    wandb.save(f"{wandb_init_params['experiment']}.onnx")
    return results
